{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.11"},"accelerator":"TPU","colab":{"gpuType":"V28","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6499184,"sourceType":"datasetVersion","datasetId":3756644},{"sourceId":12003796,"sourceType":"datasetVersion","datasetId":7551254},{"sourceId":12457906,"sourceType":"datasetVersion","datasetId":7858614}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"widgets":{"application/vnd.jupyter.widget-state+json":{"02ad2fa544aa4cce8a632248dbd92a3e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0979b9d638d44240bf1069106921f8d8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0b812ba27f7e4e999c66c24b61827fa6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f2aa4285678417ca742306b1607f2de":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d8ef59ffbaa14c6eb5118e3450cbd879","placeholder":"​","style":"IPY_MODEL_8ae20ffc24b744e983595ac03f82466f","value":"Epoch 1/10 [Train]: 100%"}},"0f501db5ae754cfd9f2723281c9cb029":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"13dd11d4a88243b7b85a51b3f7ae4828":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1a635b9bafaf4673ade9050bf8aef48a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_204142e2902d4caab49db7d97ade8bf2","IPY_MODEL_725b0971524347baae241bfb9cf5b83e","IPY_MODEL_fc34697e10204c37a6214372fa923b29"],"layout":"IPY_MODEL_f2228023f1ac4316a3239391adef8580"}},"1c061bdef2e0454fa14f3633996c1a34":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"204142e2902d4caab49db7d97ade8bf2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a14708f9a6a429c8efbc91333b78b0b","placeholder":"​","style":"IPY_MODEL_69305ad0cfa14e04b466e1e67416f3af","value":"Epoch 2/10 [Train]: 100%"}},"2a14708f9a6a429c8efbc91333b78b0b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b84663a558f425c8d8e9ad1b69b1d56":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2e7f8cf5e1f847f094abee25681347af":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_02ad2fa544aa4cce8a632248dbd92a3e","placeholder":"​","style":"IPY_MODEL_659d31d2c9ee42efb4fe5b5902f80426","value":" 8905/8905 [16:04&lt;00:00,  9.27it/s, val_loss=0.699, val_acc=0.4]"}},"38bbe05ef2ec47138231999193e67479":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b442ed8791845e4a56fee8c350ebe68","max":8905,"min":0,"orientation":"horizontal","style":"IPY_MODEL_13dd11d4a88243b7b85a51b3f7ae4828","value":8905}},"3a4af09fb3bd478082e189b2652a2981":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3b442ed8791845e4a56fee8c350ebe68":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43fada4513134e48863ea6da88b9ba11":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"457cfa5fa7a743dea134f4300bee034a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4a0d3382e71048168485a43ce39cee27":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4df60465229649e89dbe258126c2442a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a4af09fb3bd478082e189b2652a2981","placeholder":"​","style":"IPY_MODEL_457cfa5fa7a743dea134f4300bee034a","value":" 8905/8905 [16:02&lt;00:00,  9.27it/s, val_loss=0.686, val_acc=0.6]"}},"4e72cec072de4dbabc9eb842d65e52e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6b1818ae115d42ecb8f9ceb0117fe755","IPY_MODEL_679508c276f64afab0d9a434c04827f5","IPY_MODEL_2e7f8cf5e1f847f094abee25681347af"],"layout":"IPY_MODEL_9ebbc524ad754da09c68a6db61f8de31"}},"5416ee85ffdd4fc08f5a705968064d46":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"585becbbbf4f4444b30b3cacc2168435":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"616e98d2a1fd4b17878f915d1230ddfc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bcb291b83d35460cb5768620889a4d57","placeholder":"​","style":"IPY_MODEL_a8b68e114fcb4f37aff76c52e5531a5e","value":"Epoch 2/10 [Val]: 100%"}},"659d31d2c9ee42efb4fe5b5902f80426":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"679508c276f64afab0d9a434c04827f5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc4572d840b34eccba6c5765cd155a33","max":8905,"min":0,"orientation":"horizontal","style":"IPY_MODEL_78daf35001624609ab9164bbca5ecae0","value":8905}},"69305ad0cfa14e04b466e1e67416f3af":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6b1818ae115d42ecb8f9ceb0117fe755":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_947cca42eebe4d4c853c54a979002058","placeholder":"​","style":"IPY_MODEL_0f501db5ae754cfd9f2723281c9cb029","value":"Epoch 1/10 [Val]: 100%"}},"6b7ed2e49ff0453185f5900002d5b7d9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"725b0971524347baae241bfb9cf5b83e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ab91d83c0f6b4955a9c7643608c34711","max":3173,"min":0,"orientation":"horizontal","style":"IPY_MODEL_585becbbbf4f4444b30b3cacc2168435","value":3173}},"78daf35001624609ab9164bbca5ecae0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7a3f7af7da9748818730d33b017157d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_616e98d2a1fd4b17878f915d1230ddfc","IPY_MODEL_38bbe05ef2ec47138231999193e67479","IPY_MODEL_4df60465229649e89dbe258126c2442a"],"layout":"IPY_MODEL_43fada4513134e48863ea6da88b9ba11"}},"7f5e55e4044e497f9c69ba71ab6f1ff9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0f2aa4285678417ca742306b1607f2de","IPY_MODEL_803264387ea843a0bfd7762f0b38cea8","IPY_MODEL_c9557b6691ec44da8240a58f66063b78"],"layout":"IPY_MODEL_f1f6a8901c394919abcc03197ccfe61d"}},"803264387ea843a0bfd7762f0b38cea8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b21b1d683f35483799da52f4067a5aac","max":3173,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4a0d3382e71048168485a43ce39cee27","value":3173}},"83021b8ec56d43b6adf65f26f30c328a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ae20ffc24b744e983595ac03f82466f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8ccc23c62f85479390ecfdeb63910ac3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"947cca42eebe4d4c853c54a979002058":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ebbc524ad754da09c68a6db61f8de31":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a35053c82ef74b20ab304a411845e11f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea307c90e8394b7ab742629c42033a2d","max":3173,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2b84663a558f425c8d8e9ad1b69b1d56","value":2895}},"a8b68e114fcb4f37aff76c52e5531a5e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ab91d83c0f6b4955a9c7643608c34711":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b21b1d683f35483799da52f4067a5aac":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bcb291b83d35460cb5768620889a4d57":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2113ef1a95c40de8ac397f94ccf46df":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9557b6691ec44da8240a58f66063b78":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b7ed2e49ff0453185f5900002d5b7d9","placeholder":"​","style":"IPY_MODEL_c2113ef1a95c40de8ac397f94ccf46df","value":" 3173/3173 [24:47&lt;00:00,  2.50it/s, loss=0.687, acc=0.75]"}},"cda59005f6bc4844941d518e79b5aa7d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dae6ae503ed94456b11d809cf38e7d51","IPY_MODEL_a35053c82ef74b20ab304a411845e11f","IPY_MODEL_de5cf0e5bea94172b9e18ef3cd492514"],"layout":"IPY_MODEL_0b812ba27f7e4e999c66c24b61827fa6"}},"d37bb929dafa4150b4a457b1e9d1f3c5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8ef59ffbaa14c6eb5118e3450cbd879":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dae6ae503ed94456b11d809cf38e7d51":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d37bb929dafa4150b4a457b1e9d1f3c5","placeholder":"​","style":"IPY_MODEL_8ccc23c62f85479390ecfdeb63910ac3","value":"Epoch 3/10 [Train]:  91%"}},"de5cf0e5bea94172b9e18ef3cd492514":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_83021b8ec56d43b6adf65f26f30c328a","placeholder":"​","style":"IPY_MODEL_5416ee85ffdd4fc08f5a705968064d46","value":" 2895/3173 [22:15&lt;02:04,  2.24it/s, loss=0.679, acc=0.75]"}},"ea307c90e8394b7ab742629c42033a2d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1f6a8901c394919abcc03197ccfe61d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2228023f1ac4316a3239391adef8580":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc34697e10204c37a6214372fa923b29":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c061bdef2e0454fa14f3633996c1a34","placeholder":"​","style":"IPY_MODEL_0979b9d638d44240bf1069106921f8d8","value":" 3173/3173 [24:31&lt;00:00,  2.47it/s, loss=0.663, acc=0.75]"}},"fc4572d840b34eccba6c5765cd155a33":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import kagglehub\nhahunavth_asvspoof2019_la_path = kagglehub.dataset_download('hahunavth/asvspoof2019-la')\n# asvspoof2021_la_path = kagglehub.dataset_download('ajaysuryal/asvspoof2021-la')\nwav2vec_model_path = kagglehub.dataset_download('ajaysuryal/wav2vec2-base')\ntest_dataset = kagglehub.dataset_download('ajaysuryal/test-audio')\nprint('Data source import complete.')","metadata":{"id":"R4DQYTTnjI_9","outputId":"6c1e90cb-7ac5-44f9-8a9d-7853e20b0114","trusted":true,"execution":{"iopub.status.busy":"2025-07-13T14:44:56.142719Z","iopub.execute_input":"2025-07-13T14:44:56.142953Z","iopub.status.idle":"2025-07-13T14:44:56.683287Z","shell.execute_reply.started":"2025-07-13T14:44:56.142929Z","shell.execute_reply":"2025-07-13T14:44:56.682589Z"}},"outputs":[{"name":"stdout","text":"Data source import complete.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport warnings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T14:44:56.684074Z","iopub.execute_input":"2025-07-13T14:44:56.684353Z","iopub.status.idle":"2025-07-13T14:44:56.998002Z","shell.execute_reply.started":"2025-07-13T14:44:56.684291Z","shell.execute_reply":"2025-07-13T14:44:56.997360Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\nfrom torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\nfrom torch.nn.utils.rnn import pad_sequence\n\n\nimport torchaudio\nimport torchaudio.functional as AF\nimport torchaudio.functional as TF \n\nfrom transformers import Wav2Vec2Config, Wav2Vec2Model\n\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import (\n    accuracy_score,\n    precision_recall_fscore_support,\n    confusion_matrix\n)\n\ntorch.utils.data._utils.worker._worker_loop._WARNINGS_AS_ERRORS = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T14:44:56.999831Z","iopub.execute_input":"2025-07-13T14:44:57.000210Z","iopub.status.idle":"2025-07-13T14:45:22.439574Z","shell.execute_reply.started":"2025-07-13T14:44:57.000186Z","shell.execute_reply":"2025-07-13T14:45:22.438784Z"}},"outputs":[{"name":"stderr","text":"2025-07-13 14:45:11.257902: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752417911.470398      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752417911.532367      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install --upgrade numpy==1.23.5  \n!pip install numpy_minmax numpy_rms   \n!pip install audiomentations==0.28.0  #\n\nfrom audiomentations import Compose, AddGaussianNoise, PitchShift, Shift, TimeStretch, Gain, PolarityInversion, HighPassFilter, LowPassFilter, ClippingDistortion\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T14:45:22.442944Z","iopub.execute_input":"2025-07-13T14:45:22.443189Z","iopub.status.idle":"2025-07-13T14:45:45.629605Z","shell.execute_reply.started":"2025-07-13T14:45:22.443171Z","shell.execute_reply":"2025-07-13T14:45:45.628856Z"}},"outputs":[{"name":"stdout","text":"Collecting numpy==1.23.5\n  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\nDownloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.4\n    Uninstalling numpy-1.26.4:\n      Successfully uninstalled numpy-1.26.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.2 which is incompatible.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.23.5 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.23.5 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.23.5 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\nwoodwork 0.31.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\nfeaturetools 1.31.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\npyldavis 3.4.1 requires numpy>=1.24.2, but you have numpy 1.23.5 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.23.5 which is incompatible.\nbayesian-optimization 2.0.3 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\njax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\npymc 5.21.2 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nscikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\ntreescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\nbigframes 1.42.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nimbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nblosc2 3.2.1 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\nchex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\njaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\nalbumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\nxarray 2025.1.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nalbucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.23.5\nCollecting numpy_minmax\n  Downloading numpy_minmax-0.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nCollecting numpy_rms\n  Downloading numpy_rms-0.6.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\nRequirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from numpy_minmax) (1.17.1)\nCollecting numpy<3,>=2 (from numpy_minmax)\n  Downloading numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (62 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.0->numpy_minmax) (2.22)\nDownloading numpy_minmax-0.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\nDownloading numpy_rms-0.6.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17 kB)\nDownloading numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl (16.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy, numpy_rms, numpy_minmax\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.23.5\n    Uninstalling numpy-1.23.5:\n      Successfully uninstalled numpy-1.23.5\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.1 which is incompatible.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.2 which is incompatible.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.1 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.1 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.1 which is incompatible.\ncupy-cuda12x 13.4.1 requires numpy<2.3,>=1.22, but you have numpy 2.3.1 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.1 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\nydata-profiling 4.16.1 requires numpy<2.2,>=1.16.0, but you have numpy 2.3.1 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.1 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-2.3.1 numpy_minmax-0.5.0 numpy_rms-0.6.0\nCollecting audiomentations==0.28.0\n  Downloading audiomentations-0.28.0-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: numpy>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from audiomentations==0.28.0) (2.3.1)\nCollecting librosa<0.10.0,>0.7.2 (from audiomentations==0.28.0)\n  Downloading librosa-0.9.2-py3-none-any.whl.metadata (8.2 kB)\nRequirement already satisfied: scipy<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from audiomentations==0.28.0) (1.15.2)\nRequirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations==0.28.0) (3.0.1)\nRequirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations==0.28.0) (1.2.2)\nRequirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations==0.28.0) (1.5.0)\nRequirement already satisfied: decorator>=4.0.10 in /usr/local/lib/python3.11/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations==0.28.0) (4.4.2)\nCollecting resampy>=0.2.2 (from librosa<0.10.0,>0.7.2->audiomentations==0.28.0)\n  Downloading resampy-0.4.3-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: numba>=0.45.1 in /usr/local/lib/python3.11/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations==0.28.0) (0.60.0)\nRequirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.11/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations==0.28.0) (0.13.1)\nRequirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations==0.28.0) (1.8.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from librosa<0.10.0,>0.7.2->audiomentations==0.28.0) (25.0)\nRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.45.1->librosa<0.10.0,>0.7.2->audiomentations==0.28.0) (0.43.0)\nCollecting numpy>=1.13.0 (from audiomentations==0.28.0)\n  Downloading numpy-2.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations==0.28.0) (4.3.8)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations==0.28.0) (2.32.3)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.19.1->librosa<0.10.0,>0.7.2->audiomentations==0.28.0) (3.6.0)\nRequirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.10.2->librosa<0.10.0,>0.7.2->audiomentations==0.28.0) (1.17.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa<0.10.0,>0.7.2->audiomentations==0.28.0) (2.22)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations==0.28.0) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations==0.28.0) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations==0.28.0) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa<0.10.0,>0.7.2->audiomentations==0.28.0) (2025.4.26)\nDownloading audiomentations-0.28.0-py3-none-any.whl (66 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.0/66.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading librosa-0.9.2-py3-none-any.whl (214 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.3/214.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy-2.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading resampy-0.4.3-py3-none-any.whl (3.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy, resampy, librosa, audiomentations\n  Attempting uninstall: numpy\n    Found existing installation: numpy 2.3.1\n    Uninstalling numpy-2.3.1:\n      Successfully uninstalled numpy-2.3.1\n  Attempting uninstall: librosa\n    Found existing installation: librosa 0.11.0\n    Uninstalling librosa-0.11.0:\n      Successfully uninstalled librosa-0.11.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.0.2 which is incompatible.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.2 which is incompatible.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.0.2 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.0.2 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.0.2 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nimbalanced-learn 0.13.0 requires scikit-learn<2,>=1.3.2, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed audiomentations-0.28.0 librosa-0.9.2 numpy-2.0.2 resampy-0.4.3\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from pathlib import Path\n\ninput_path = Path(test_dataset + '/test_audio' )\nif input_path.exists():\n    print(\"\\nAlso found dataset in /kaggle/input:\")\n    for item in input_path.iterdir():\n        print(f\" - {item.name}\")\nelse:\n  print(\"not exits\")","metadata":{"id":"RRLfwWJBoZp-","outputId":"6077528a-023a-481c-f2c8-fde1b5853734","trusted":true,"execution":{"iopub.status.busy":"2025-07-13T14:45:45.630573Z","iopub.execute_input":"2025-07-13T14:45:45.631497Z","iopub.status.idle":"2025-07-13T14:45:45.640528Z","shell.execute_reply.started":"2025-07-13T14:45:45.631469Z","shell.execute_reply":"2025-07-13T14:45:45.639221Z"}},"outputs":[{"name":"stdout","text":"\nAlso found dataset in /kaggle/input:\n - 1_fake.wav\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T14:45:45.641745Z","iopub.execute_input":"2025-07-13T14:45:45.642091Z","iopub.status.idle":"2025-07-13T14:45:45.706745Z","shell.execute_reply.started":"2025-07-13T14:45:45.642062Z","shell.execute_reply":"2025-07-13T14:45:45.705636Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"path_2019 = hahunavth_asvspoof2019_la_path+'/LA/ASVspoof2019_LA_train/flac'\neval_path =hahunavth_asvspoof2019_la_path +  '/LA/ASVspoof2019_LA_eval/flac'\n\n\ntrain_protocol = hahunavth_asvspoof2019_la_path+'/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt'\neval_protocol =  hahunavth_asvspoof2019_la_path +'/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.eval.trl.txt'","metadata":{"id":"SdbtUbs2ok9B","trusted":true,"execution":{"iopub.status.busy":"2025-07-13T14:45:45.707512Z","iopub.execute_input":"2025-07-13T14:45:45.707861Z","iopub.status.idle":"2025-07-13T14:45:45.721700Z","shell.execute_reply.started":"2025-07-13T14:45:45.707828Z","shell.execute_reply":"2025-07-13T14:45:45.720963Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# path_2019 = '/kaggle/input/asvspoof2019-la/LA/ASVspoof2019_LA_train/flac'\n# eval_path = \"/kaggle/input/asvspoof2019-la/LA/ASVspoof2019_LA_eval/flac\"\n# train_protocol = '/kaggle/input/asvspoof2019-la/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.train.trn.txt'\n# eval_protocol = '/kaggle/input/asvspoof2019-la/LA/ASVspoof2019_LA_cm_protocols/ASVspoof2019.LA.cm.eval.trl.txt'","metadata":{"id":"remsusznjJAC","trusted":true,"execution":{"iopub.status.busy":"2025-07-13T14:45:45.724657Z","iopub.execute_input":"2025-07-13T14:45:45.724878Z","iopub.status.idle":"2025-07-13T14:45:45.735024Z","shell.execute_reply.started":"2025-07-13T14:45:45.724861Z","shell.execute_reply":"2025-07-13T14:45:45.734343Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"augment = Compose([\n    AddGaussianNoise(\n        min_amplitude=0.001, \n        max_amplitude=0.02, \n        p=0.3\n    ),\n    PitchShift(\n        min_semitones=-4, \n        max_semitones=4, \n        p=0.4\n    ),\n    Shift(\n        min_fraction=-0.5, \n        max_fraction=0.5, \n        p=0.3\n    ),\n    TimeStretch(\n        min_rate=0.8, \n        max_rate=1.2, \n        p=0.3\n    ),\n    HighPassFilter(\n        min_cutoff_freq=50, \n        max_cutoff_freq=200, \n        p=0.2\n    ),\n    LowPassFilter(\n        min_cutoff_freq=3000, \n        max_cutoff_freq=8000, \n        p=0.2\n    ),\n    Gain(\n        min_gain_in_db=-6, \n        max_gain_in_db=6, \n        p=0.3\n    ),\n    ClippingDistortion(\n        min_percentile_threshold=0,\n        max_percentile_threshold=10,\n        p=0.2\n    ),\n    PolarityInversion(p=0.1)\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T14:45:45.735863Z","iopub.execute_input":"2025-07-13T14:45:45.736204Z","iopub.status.idle":"2025-07-13T14:45:45.747734Z","shell.execute_reply.started":"2025-07-13T14:45:45.736178Z","shell.execute_reply":"2025-07-13T14:45:45.747124Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# For your training dataset\ntrain_df = pd.read_csv(train_protocol, sep=' ', header=None,\n                     names=['speaker_id', 'audio_id', 'environment', 'attack_id', 'label'])\n\n# Count the occurrences of each class\nclass_counts = train_df['label'].value_counts()\nprint(\"Training set class distribution:\")\nprint(class_counts)\n\n# Calculate percentages\nclass_percentages = train_df['label'].value_counts(normalize=True) * 100\nprint(\"\\nClass percentages:\")\nprint(class_percentages)","metadata":{"id":"h62ccU9XjJAE","outputId":"30fb3406-4d3d-46fb-8bcd-2a2ac438c282","trusted":true,"execution":{"iopub.status.busy":"2025-07-13T14:45:45.748424Z","iopub.execute_input":"2025-07-13T14:45:45.748625Z","iopub.status.idle":"2025-07-13T14:45:45.811915Z","shell.execute_reply.started":"2025-07-13T14:45:45.748612Z","shell.execute_reply":"2025-07-13T14:45:45.811295Z"}},"outputs":[{"name":"stdout","text":"Training set class distribution:\nlabel\nspoof       22800\nbonafide     2580\nName: count, dtype: int64\n\nClass percentages:\nlabel\nspoof       89.834515\nbonafide    10.165485\nName: proportion, dtype: float64\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"MAX_T = 48000","metadata":{"id":"QHt_KZU-jJAE","trusted":true,"execution":{"iopub.status.busy":"2025-07-13T14:45:45.812603Z","iopub.execute_input":"2025-07-13T14:45:45.812837Z","iopub.status.idle":"2025-07-13T14:45:45.816546Z","shell.execute_reply.started":"2025-07-13T14:45:45.812821Z","shell.execute_reply":"2025-07-13T14:45:45.815861Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class AudioSpoofDataset(Dataset):\n    def __init__(self, csv_path, audio_root, sample_rate=16000, augment_fn=None):\n        import pandas as pd\n        self.df = pd.read_csv(csv_path, sep=' ', header=None,\n                              names=['speaker_id', 'audio_id', 'environment', 'attack_id', 'label'])\n        self.audio_root = audio_root\n        self.sample_rate = sample_rate\n        self.augment_fn = augment_fn\n        self.label_map = {'bonafide': 0, 'spoof': 1}\n        self.MAX_T = 48000\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        import os\n        row = self.df.iloc[idx]\n        audio_path = os.path.join(self.audio_root, f\"{row.audio_id}.flac\")\n        waveform, sr = torchaudio.load(audio_path)\n        if sr != self.sample_rate:\n            waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=self.sample_rate)(waveform)\n        if waveform.size(-1) > self.MAX_T:\n            waveform = waveform[:, :self.MAX_T]\n        elif waveform.size(-1) < self.MAX_T:\n            pad = self.MAX_T - waveform.size(-1)\n            waveform = F.pad(waveform, (0, pad))\n        waveform = waveform.squeeze(0).float()  # [T]\n        if self.augment_fn:\n            wav_np = waveform.numpy()\n            wav_aug = self.augment_fn(samples=wav_np, sample_rate=self.sample_rate)\n            waveform = torch.tensor(wav_aug)\n        return {\n            'waveform': waveform,  # [T]\n            'label': torch.tensor(self.label_map[row.label], dtype=torch.long),\n            'audio_id': row.audio_id }","metadata":{"id":"0m3UvFBejJAE","trusted":true,"execution":{"iopub.status.busy":"2025-07-13T14:45:45.817436Z","iopub.execute_input":"2025-07-13T14:45:45.817786Z","iopub.status.idle":"2025-07-13T14:45:45.830423Z","shell.execute_reply.started":"2025-07-13T14:45:45.817759Z","shell.execute_reply":"2025-07-13T14:45:45.829716Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def collate_fn(batch):\n    waveforms = torch.stack([item['waveform'] for item in batch])\n    labels = torch.stack([item['label'] for item in batch])\n    return {\n        'waveform': waveforms,        # [B, 1, T]\n        'label': labels\n    }","metadata":{"id":"7EhnxRgrjJAF","trusted":true,"execution":{"iopub.status.busy":"2025-07-13T14:45:45.831123Z","iopub.execute_input":"2025-07-13T14:45:45.831398Z","iopub.status.idle":"2025-07-13T14:45:45.845101Z","shell.execute_reply.started":"2025-07-13T14:45:45.831374Z","shell.execute_reply":"2025-07-13T14:45:45.844440Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"train_dataset = AudioSpoofDataset(\n    csv_path= train_protocol,\n    audio_root= path_2019,\n    sample_rate=16000,\n    augment_fn=augment\n)","metadata":{"id":"2Rfd04LwjJAF","trusted":true,"execution":{"iopub.status.busy":"2025-07-13T14:45:45.845882Z","iopub.execute_input":"2025-07-13T14:45:45.846093Z","iopub.status.idle":"2025-07-13T14:45:45.879909Z","shell.execute_reply.started":"2025-07-13T14:45:45.846074Z","shell.execute_reply":"2025-07-13T14:45:45.879361Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"labels = train_df['label'].map({'spoof': 1, 'bonafide': 0}).values\nclass_counts = np.bincount(labels)\nweights = 1. / class_counts[labels]  # inverse freq\nsampler = WeightedRandomSampler(weights, num_samples=len(weights), replacement=True)\n\ntrain_loader = DataLoader(train_dataset, batch_size=8, sampler=sampler, collate_fn=collate_fn)","metadata":{"id":"uJXsVUnQ4jkv","trusted":true,"execution":{"iopub.status.busy":"2025-07-13T14:45:45.880626Z","iopub.execute_input":"2025-07-13T14:45:45.880817Z","iopub.status.idle":"2025-07-13T14:45:45.891032Z","shell.execute_reply.started":"2025-07-13T14:45:45.880803Z","shell.execute_reply":"2025-07-13T14:45:45.890270Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"eval_dataset = AudioSpoofDataset(\n    csv_path= eval_protocol,\n    audio_root= eval_path,\n    sample_rate=16000,\n    augment_fn=None\n)","metadata":{"id":"UjLxpwIsjJAF","trusted":true,"execution":{"iopub.status.busy":"2025-07-13T14:45:45.891785Z","iopub.execute_input":"2025-07-13T14:45:45.892009Z","iopub.status.idle":"2025-07-13T14:45:45.979898Z","shell.execute_reply.started":"2025-07-13T14:45:45.891987Z","shell.execute_reply":"2025-07-13T14:45:45.979332Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# eval_loader = DataLoader(\n#     eval_dataset,\n#     batch_size=8,\n#     shuffle=False,\n#     num_workers=2,\n#     collate_fn=collate_fn\n# )","metadata":{"id":"5d2goZnXjJAF","trusted":true,"execution":{"iopub.status.busy":"2025-07-13T14:45:45.980557Z","iopub.execute_input":"2025-07-13T14:45:45.980808Z","iopub.status.idle":"2025-07-13T14:45:45.984145Z","shell.execute_reply.started":"2025-07-13T14:45:45.980788Z","shell.execute_reply":"2025-07-13T14:45:45.983491Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"from torch.utils.data import random_split\n\ntotal_size = len(eval_dataset)\nsplit_1_size = int(0.7 * total_size)\nsplit_2_size = total_size - split_1_size\n\n# Create the splits\neval_dataset_70, eval_dataset_30 = random_split(\n    eval_dataset, \n    [split_1_size, split_2_size]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T14:45:45.984946Z","iopub.execute_input":"2025-07-13T14:45:45.985196Z","iopub.status.idle":"2025-07-13T14:45:46.001493Z","shell.execute_reply.started":"2025-07-13T14:45:45.985174Z","shell.execute_reply":"2025-07-13T14:45:46.000747Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Create the splits\neval_dataset_70, eval_dataset_30 = random_split(\n    eval_dataset, \n    [split_1_size, split_2_size]\n)\n\n# Create DataLoaders for each subset if needed\neval_loader = DataLoader(\n    eval_dataset_70,\n    batch_size=8,\n    shuffle=False,\n    num_workers=2,\n    collate_fn=collate_fn\n)\n\neval_loader_test = DataLoader(\n    eval_dataset_30,\n    batch_size=8,\n    shuffle=False,\n    num_workers=2,\n    collate_fn=collate_fn\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T14:45:46.002271Z","iopub.execute_input":"2025-07-13T14:45:46.002508Z","iopub.status.idle":"2025-07-13T14:45:46.012839Z","shell.execute_reply.started":"2025-07-13T14:45:46.002492Z","shell.execute_reply":"2025-07-13T14:45:46.011953Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"batch = next(iter(train_loader))\nprint(\"Waveform shape:\", batch['waveform'].shape)\nprint(\"Labels shape:\", batch['label'].shape)","metadata":{"id":"JQMJ5Xw5jJAG","outputId":"d0be0fbc-3521-421d-a9fa-2609dc28404b","trusted":true,"execution":{"iopub.status.busy":"2025-07-13T14:45:46.013620Z","iopub.execute_input":"2025-07-13T14:45:46.013814Z","iopub.status.idle":"2025-07-13T14:45:48.268649Z","shell.execute_reply.started":"2025-07-13T14:45:46.013800Z","shell.execute_reply":"2025-07-13T14:45:48.267878Z"}},"outputs":[{"name":"stdout","text":"Waveform shape: torch.Size([8, 48000])\nLabels shape: torch.Size([8])\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"for batch_idx, batch in enumerate(eval_loader):\n    print(f\"\\nBatch {batch_idx}:\")\n    print(\"- Waveform shape:\", batch['waveform'].shape)\n    print(\"- Labels shape:\", batch['label'].shape)\n    print(\"- Sample audio_id:\", train_dataset.df.iloc[batch_idx * 8]['audio_id'])  # First sample's ID\n\n    # Check first sample's stats\n    sample_wave = batch['waveform'][0]  # First sample [1, T]\n    print(\"- Audio length (samples):\", sample_wave.shape[-1])\n    print(\"- Sample rate:\", train_dataset.sample_rate)\n    print(\"- Label:\", batch['label'][0].item())\n\n    break","metadata":{"id":"FrjPwZhHjJAG","outputId":"b34a8321-017b-4fdc-ad88-092ccb2baf92","trusted":true,"execution":{"iopub.status.busy":"2025-07-13T14:45:48.269762Z","iopub.execute_input":"2025-07-13T14:45:48.270039Z","iopub.status.idle":"2025-07-13T14:45:48.575978Z","shell.execute_reply.started":"2025-07-13T14:45:48.270013Z","shell.execute_reply":"2025-07-13T14:45:48.575022Z"}},"outputs":[{"name":"stdout","text":"\nBatch 0:\n- Waveform shape: torch.Size([8, 48000])\n- Labels shape: torch.Size([8])\n- Sample audio_id: LA_T_1138215\n- Audio length (samples): 48000\n- Sample rate: 16000\n- Label: 1\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"transformers\")","metadata":{"id":"MMlUlnQyjJAG","trusted":true,"execution":{"iopub.status.busy":"2025-07-13T14:45:48.577275Z","iopub.execute_input":"2025-07-13T14:45:48.577646Z","iopub.status.idle":"2025-07-13T14:45:48.582717Z","shell.execute_reply.started":"2025-07-13T14:45:48.577611Z","shell.execute_reply":"2025-07-13T14:45:48.581903Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"class AntiSpoofModel(nn.Module):\n    def __init__(self, wav2vec_model_path, use_transformer=True, num_classes=2):\n        super().__init__()\n        config = Wav2Vec2Config.from_pretrained(\n            wav2vec_model_path,\n            hidden_dropout=0.1,\n            attention_dropout=0.1\n        )\n        self.wav2vec = Wav2Vec2Model.from_pretrained(wav2vec_model_path, config=config)\n        self.wav2vec.feature_extractor._freeze_parameters()  # Freeze low-level features\n        \n        # 1. MULTI-SCALE F0 AND SPECTRAL ANALYSIS\n        # Multiple bandpass filters for different frequency ranges\n        self.f0_low = self._create_bandpass_filter(768, 50, 200)    # Low F0\n        self.f0_mid = self._create_bandpass_filter(768, 200, 500)   # Mid F0\n        self.f0_high = self._create_bandpass_filter(768, 500, 1000) # High F0\n        \n        # Spectral inconsistency detector\n        self.spectral_analyzer = nn.Sequential(\n            nn.Conv1d(768, 384, kernel_size=7, padding=3),\n            nn.BatchNorm1d(384),\n            nn.ReLU(),\n            nn.Conv1d(384, 192, kernel_size=5, padding=2),\n        )\n        \n        # 2. ENHANCED CNN WITH RESIDUAL CONNECTIONS\n        # Multi-branch CNN for different temporal scales\n        self.cnn_short = self._create_cnn_branch(768 + 192, 256, [3, 5])     \n        self.cnn_medium = self._create_cnn_branch(768 + 192, 256, [7, 11])    \n        self.cnn_long = self._create_cnn_branch(768 + 192, 256, [15, 21])     \n        \n        # Feature fusion\n        self.feature_fusion = nn.Sequential(\n            nn.Conv1d(768, 256, kernel_size=1),  # Dimension reduction\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.Dropout(0.2)\n        )\n        \n        # 3. ATTENTION MECHANISMS\n        # Self-attention for temporal dependencies\n        self.temporal_attention = nn.MultiheadAttention(\n            embed_dim=256, num_heads=8, dropout=0.1, batch_first=True\n        )\n        \n        # Channel attention for feature importance\n        self.channel_attention = nn.Sequential(\n            nn.AdaptiveAvgPool1d(1),\n            nn.Conv1d(256, 64, 1),\n            nn.ReLU(),\n            nn.Conv1d(64, 256, 1),\n            nn.Sigmoid()\n        )\n        \n        # 4. IMPROVED TRANSFORMER\n        if use_transformer:\n            encoder_layer = nn.TransformerEncoderLayer(\n                d_model=256, nhead=8, dim_feedforward=1024, \n                dropout=0.1, batch_first=True\n            )\n            self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=3)\n        else:\n            self.transformer = nn.Identity()\n        \n        # 5. SOPHISTICATED POOLING AND CLASSIFICATION\n        # Multiple pooling strategies\n        self.adaptive_pool = nn.AdaptiveAvgPool1d(1)\n        self.max_pool = nn.AdaptiveMaxPool1d(1)\n        \n        # Classification head with uncertainty estimation\n        self.head = nn.Sequential(\n            nn.Linear(256 * 2, 512),  # *2 for avg + max pooling\n            nn.LayerNorm(512),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(512, 256),\n            nn.LayerNorm(256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, num_classes)\n        )\n        \n        # Uncertainty head for confidence estimation\n        self.uncertainty_head = nn.Sequential(\n            nn.Linear(256 * 2, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1),\n            nn.Sigmoid()\n        )\n    \n    def _create_bandpass_filter(self, channels, low_freq, high_freq):\n        \"\"\"Create a learnable bandpass filter\"\"\"\n        conv = nn.Conv1d(channels, channels, kernel_size=7, padding=3, groups=channels, bias=False)\n        # Initialize with bandpass characteristics\n        with torch.no_grad():\n            # Simple bandpass initialization (can be improved with actual filter design)\n            conv.weight.fill_(1/7)\n        return conv\n    \n    def _create_cnn_branch(self, in_channels, out_channels, kernel_sizes):\n        \"\"\"Create a multi-scale CNN branch\"\"\"\n        layers = []\n        current_channels = in_channels\n        \n        for i, k in enumerate(kernel_sizes):\n            layers.extend([\n                nn.Conv1d(current_channels, out_channels // len(kernel_sizes), \n                         kernel_size=k, padding=k//2),\n                nn.BatchNorm1d(out_channels // len(kernel_sizes)),\n                nn.ReLU(),\n                nn.Dropout(0.1)\n            ])\n            if i == 0:\n                current_channels = out_channels // len(kernel_sizes)\n        \n        layers.append(nn.MaxPool1d(kernel_size=2))\n        return nn.Sequential(*layers)\n    \n    def forward(self, waveform):\n        # waveform: [B, T]\n        x = self.wav2vec(waveform).last_hidden_state  # [B, T', 768]\n        x = x.transpose(1, 2)  # [B, 768, T']\n        \n        # 1. Multi-scale F0 analysis\n        f0_low = self.f0_low(x)\n        f0_mid = self.f0_mid(x)  \n        f0_high = self.f0_high(x)\n        \n        # Combine F0 features\n        f0_combined = f0_low + f0_mid + f0_high\n        \n        # 2. Spectral inconsistency detection\n        spectral_features = self.spectral_analyzer(x)\n        \n        # 3. Combine features\n        combined_features = torch.cat([x, spectral_features], dim=1)  # [B, 768+192, T']\n        \n        # 4. Multi-scale CNN processing\n        short_features = self.cnn_short(combined_features)\n        medium_features = self.cnn_medium(combined_features)\n        long_features = self.cnn_long(combined_features)\n        \n        # Combine multi-scale features\n        multi_scale = torch.cat([short_features, medium_features, long_features], dim=1)\n        \n        # 5. Feature fusion and dimension reduction\n        x = self.feature_fusion(f0_combined + x)  # Residual connection\n        \n        # 6. Channel attention\n        channel_weights = self.channel_attention(x)\n        x = x * channel_weights\n        \n        # 7. Temporal attention\n        x = x.transpose(1, 2)  # [B, T'', 256]\n        x_attended, _ = self.temporal_attention(x, x, x)\n        x = x + x_attended  # Residual connection\n        \n        # 8. Transformer processing\n        x = self.transformer(x)  # [B, T'', 256]\n        \n        # 9. Advanced pooling\n        x = x.transpose(1, 2)  # [B, 256, T'']\n        avg_pooled = self.adaptive_pool(x).squeeze(-1)  # [B, 256]\n        max_pooled = self.max_pool(x).squeeze(-1)       # [B, 256]\n        \n        # Combine pooling strategies\n        pooled_features = torch.cat([avg_pooled, max_pooled], dim=1)  # [B, 512]\n        \n        # 10. Classification with uncertainty\n        logits = self.head(pooled_features)\n        confidence = self.uncertainty_head(pooled_features)\n        \n        return {\n            'logits': logits,\n            'confidence': confidence,\n            'features': pooled_features\n        }\n\n# TRAINING IMPROVEMENTS\nclass FocalLoss(nn.Module):\n    \"\"\"Focal Loss for handling class imbalance\"\"\"\n    def __init__(self, alpha=1, gamma=2):\n        super().__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        \n    def forward(self, inputs, targets):\n        ce_loss = F.cross_entropy(inputs, targets, reduction='none')\n        pt = torch.exp(-ce_loss)\n        focal_loss = self.alpha * (1-pt)**self.gamma * ce_loss\n        return focal_loss.mean()\n\nclass LabelSmoothingLoss(nn.Module):\n    \"\"\"Label smoothing for better generalization\"\"\"\n    def __init__(self, num_classes, smoothing=0.1):\n        super().__init__()\n        self.num_classes = num_classes\n        self.smoothing = smoothing\n        \n    def forward(self, inputs, targets):\n        confidence = 1.0 - self.smoothing\n        smooth_targets = torch.full_like(inputs, self.smoothing / (self.num_classes - 1))\n        smooth_targets.scatter_(1, targets.unsqueeze(1), confidence)\n        return F.kl_div(F.log_softmax(inputs, dim=1), smooth_targets, reduction='batchmean')\n\n# ENSEMBLE PREDICTION\nclass EnsemblePredictor:\n    def __init__(self, models):\n        self.models = models\n    \n    def predict(self, waveform):\n        predictions = []\n        confidences = []\n        \n        for model in self.models:\n            with torch.no_grad():\n                output = model(waveform)\n                predictions.append(F.softmax(output['logits'], dim=1))\n                confidences.append(output['confidence'])\n        \n        # Weighted average based on confidence\n        weights = torch.stack(confidences, dim=0)\n        weights = F.softmax(weights, dim=0)\n        \n        ensemble_pred = sum(w * p for w, p in zip(weights, predictions))\n        return ensemble_pred","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T14:45:48.583610Z","iopub.execute_input":"2025-07-13T14:45:48.583890Z","iopub.status.idle":"2025-07-13T14:45:48.610721Z","shell.execute_reply.started":"2025-07-13T14:45:48.583869Z","shell.execute_reply":"2025-07-13T14:45:48.610067Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"model = AntiSpoofModel(\n    wav2vec_model_path=wav2vec_model_path + \"/wav2vec2-base\",\n    use_transformer=True,\n    num_classes=2\n).to(device)\n","metadata":{"id":"xkgxyOddjJAH","trusted":true,"execution":{"iopub.status.busy":"2025-07-13T14:45:48.611540Z","iopub.execute_input":"2025-07-13T14:45:48.612069Z","iopub.status.idle":"2025-07-13T14:45:51.208660Z","shell.execute_reply.started":"2025-07-13T14:45:48.612049Z","shell.execute_reply":"2025-07-13T14:45:51.207831Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*can only test a child process.*\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-13T14:45:51.209560Z","iopub.execute_input":"2025-07-13T14:45:51.210095Z","iopub.status.idle":"2025-07-13T14:45:51.214095Z","shell.execute_reply.started":"2025-07-13T14:45:51.210067Z","shell.execute_reply":"2025-07-13T14:45:51.213475Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# Initialize the model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n\n\n\nprint(f\"Model initialized with {sum(p.numel() for p in model.parameters()):,} parameters\")\nprint(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n\n# ADVANCED TRAINING SETUP\nclass ImprovedTrainer:\n    def __init__(self, model, device, use_focal_loss=True, use_label_smoothing=False):\n        self.model = model\n        self.device = device\n        \n        # Choose loss function based on your data distribution\n        self.criterion = LabelSmoothingLoss(num_classes=2, smoothing=0.1)\n\n        # ...existing code...\n        wav2vec_params = list(self.model.wav2vec.parameters())\n        wav2vec_param_ids = set(id(p) for p in wav2vec_params)\n        other_params = [p for p in self.model.parameters() if id(p) not in wav2vec_param_ids]\n        # ...existing code... \n        \n        self.optimizer = optim.AdamW([\n            {'params': wav2vec_params, 'lr': 1e-5, 'weight_decay': 1e-6},  # Lower LR for pre-trained\n            {'params': other_params, 'lr': 1e-4, 'weight_decay': 1e-5}     # Higher LR for new layers\n        ])\n        \n        # Cosine annealing with warm restarts\n        self.scheduler = CosineAnnealingWarmRestarts(\n            self.optimizer, T_0=5, T_mult=2, eta_min=1e-7\n        )\n        \n        # Backup scheduler\n        self.plateau_scheduler = ReduceLROnPlateau(\n            self.optimizer, mode='min', factor=0.5, patience=3, verbose=True\n        )\n        \n        self.best_val_acc = 0.0\n        self.best_val_loss = float('inf')\n        self.train_losses = []\n        self.val_losses = []\n        self.train_accs = []\n        self.val_accs = []\n    \n    def train_epoch(self, train_loader, epoch):\n        self.model.train()\n        train_loss, train_correct, train_total = 0, 0, 0\n        all_preds, all_labels = [], []\n        \n        train_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\")\n        \n        for batch_idx, batch in enumerate(train_iter):\n            wave = batch['waveform'].to(self.device)\n            label = batch['label'].to(self.device)\n            \n            # Handle different input shapes\n            if wave.dim() == 3 and wave.size(1) == 1:\n                wave = wave.squeeze(1)  # [B, 1, T] -> [B, T]\n            \n            self.optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = self.model(wave)\n            logits = outputs['logits'] if isinstance(outputs, dict) else outputs\n            \n            # Calculate loss\n            loss = self.criterion(logits, label)\n            \n            # Add confidence regularization if available\n            if isinstance(outputs, dict) and 'confidence' in outputs:\n                confidence_loss = 0.1 * torch.mean((outputs['confidence'] - 0.5) ** 2)\n                loss += confidence_loss\n            \n            # Backward pass\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n            self.optimizer.step()\n            \n            # Statistics\n            preds = logits.argmax(dim=1)\n            correct = (preds == label).sum().item()\n            \n            train_loss += loss.item() * wave.size(0)\n            train_correct += correct\n            train_total += label.size(0)\n            \n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(label.cpu().numpy())\n            \n            # Update progress bar\n            current_acc = correct / label.size(0)\n            train_iter.set_postfix({\n                'loss': f'{loss.item():.4f}',\n                'acc': f'{current_acc:.4f}',\n                'lr': f'{self.optimizer.param_groups[0][\"lr\"]:.2e}'\n            })\n        \n        avg_train_loss = train_loss / train_total\n        train_acc = train_correct / train_total\n        \n        # Calculate detailed metrics\n        precision, recall, f1, _ = precision_recall_fscore_support(\n            all_labels, all_preds, average='weighted'\n        )\n        \n        return avg_train_loss, train_acc, precision, recall, f1\n    \n    def validate_epoch(self, val_loader, epoch):\n        self.model.eval()\n        val_loss, val_correct, val_total = 0, 0, 0\n        all_preds, all_labels, all_confidences = [], [], []\n        \n        with torch.no_grad():\n            val_iter = tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\")\n            \n            for batch in val_iter:\n                wave = batch['waveform'].to(self.device)\n                label = batch['label'].to(self.device)\n                \n                if wave.dim() == 3 and wave.size(1) == 1:\n                    wave = wave.squeeze(1)\n                \n                outputs = self.model(wave)\n                logits = outputs['logits'] if isinstance(outputs, dict) else outputs\n                \n                loss = self.criterion(logits, label)\n                \n                preds = logits.argmax(dim=1)\n                correct = (preds == label).sum().item()\n                \n                val_loss += loss.item() * wave.size(0)\n                val_correct += correct\n                val_total += label.size(0)\n                \n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(label.cpu().numpy())\n                \n                if isinstance(outputs, dict) and 'confidence' in outputs:\n                    all_confidences.extend(outputs['confidence'].cpu().numpy())\n                \n                val_iter.set_postfix({\n                    'val_loss': f'{loss.item():.4f}',\n                    'val_acc': f'{correct / label.size(0):.4f}'\n                })\n        \n        avg_val_loss = val_loss / val_total\n        val_acc = val_correct / val_total\n        \n        # Detailed metrics\n        precision, recall, f1, _ = precision_recall_fscore_support(\n            all_labels, all_preds, average='weighted'\n        )\n        \n        # Confusion matrix\n        cm = confusion_matrix(all_labels, all_preds)\n        \n        return avg_val_loss, val_acc, precision, recall, f1, cm, all_confidences\n    \n    def train(self, train_loader, val_loader, epochs=5, save_path='best_antispoofmodel.pth'):\n        print(\"Starting training...\")\n        print(f\"Dataset size: Train={len(train_loader.dataset)}, Val={len(val_loader.dataset)}\")\n        \n        for epoch in range(epochs):\n            print(f\"\\n{'='*50}\")\n            print(f\"EPOCH {epoch+1}/{epochs}\")\n            print(f\"{'='*50}\")\n            \n            # Training\n            train_loss, train_acc, train_prec, train_rec, train_f1 = self.train_epoch(train_loader, epoch)\n            \n            # Validation\n            val_loss, val_acc, val_prec, val_rec, val_f1, cm, confidences = self.validate_epoch(val_loader, epoch)\n            \n            # Learning rate scheduling\n            self.scheduler.step()\n            # self.plateau_scheduler.step(val_loss)  # Uncomment if you want backup scheduler\n            \n            # Store metrics\n            self.train_losses.append(train_loss)\n            self.val_losses.append(val_loss)\n            self.train_accs.append(train_acc)\n            self.val_accs.append(val_acc)\n            \n            # Print detailed results\n            print(f\"\\nTRAIN METRICS:\")\n            print(f\"Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | Prec: {train_prec:.4f} | Rec: {train_rec:.4f} | F1: {train_f1:.4f}\")\n            \n            print(f\"\\nVALIDATION METRICS:\")\n            print(f\"Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | Prec: {val_prec:.4f} | Rec: {val_rec:.4f} | F1: {val_f1:.4f}\")\n            \n            print(f\"\\nConfusion Matrix:\")\n            print(f\"[[{cm[0,0]:4d}, {cm[0,1]:4d}]]  <- Real\")\n            print(f\"[[{cm[1,0]:4d}, {cm[1,1]:4d}]]  <- Fake\")\n            \n            if confidences:\n                print(f\"Average Confidence: {np.mean(confidences):.4f}\")\n            \n            # Save best model\n            if val_acc > self.best_val_acc:\n                self.best_val_acc = val_acc\n                self.best_val_loss = val_loss\n                \n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': self.model.state_dict(),\n                    'optimizer_state_dict': self.optimizer.state_dict(),\n                    'scheduler_state_dict': self.scheduler.state_dict(),\n                    'val_acc': val_acc,\n                    'val_loss': val_loss,\n                    'train_acc': train_acc,\n                    'train_loss': train_loss\n                }, save_path)\n                \n                print(f\"🎉 NEW BEST MODEL SAVED! Val Acc: {val_acc:.4f}\")\n            \n            # Early stopping check\n            if epoch > 5 and val_acc < self.best_val_acc - 0.05:\n                print(\"⚠️  Performance degrading, consider early stopping\")\n        \n        print(f\"\\n🏁 TRAINING COMPLETED!\")\n        print(f\"Best Validation Accuracy: {self.best_val_acc:.4f}\")\n        return self.best_val_acc\n    \n    def plot_training_curves(self):\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n        \n        # Loss curves\n        ax1.plot(self.train_losses, label='Train Loss', color='blue')\n        ax1.plot(self.val_losses, label='Val Loss', color='red')\n        ax1.set_xlabel('Epoch')\n        ax1.set_ylabel('Loss')\n        ax1.set_title('Training and Validation Loss')\n        ax1.legend()\n        ax1.grid(True)\n        \n        # Accuracy curves\n        ax2.plot(self.train_accs, label='Train Acc', color='blue')\n        ax2.plot(self.val_accs, label='Val Acc', color='red')\n        ax2.set_xlabel('Epoch')\n        ax2.set_ylabel('Accuracy')\n        ax2.set_title('Training and Validation Accuracy')\n        ax2.legend()\n        ax2.grid(True)\n        \n        plt.tight_layout()\n        plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')\n        plt.show()\n\n# INITIALIZE AND TRAIN\ntrainer = ImprovedTrainer(\n    model=model, \n    device=device, \n    use_focal_loss=True,  # Good for balanced datasets\n    use_label_smoothing=False\n)\n\n# Start training (replace with your actual data loaders)\nbest_accuracy = trainer.train(\n    train_loader=train_loader,  # Your train loader\n    val_loader=eval_loader,     # Your validation loader\n    epochs=3,\n    save_path='best_improved_antispoofmodel.pth'\n)\n\n# Plot training curves\ntrainer.plot_training_curves()\n\nprint(f\"\\n🎯 FINAL RESULTS:\")\nprint(f\"Best Validation Accuracy: {best_accuracy:.4f}\")\nprint(f\"Expected accuracy with 26K samples: 90-95%+ 🚀\")","metadata":{"id":"IFSEfMXlhaeB","trusted":true,"execution":{"iopub.status.busy":"2025-07-13T14:45:51.218384Z","iopub.execute_input":"2025-07-13T14:45:51.218601Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nModel initialized with 103,827,459 parameters\nTrainable parameters: 99,627,011\nStarting training...\nDataset size: Train=25380, Val=49865\n\n==================================================\nEPOCH 1/5\n==================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1 [Train]:   0%|          | 0/3173 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32959e6298b44660a2fb13a48c7610e5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 1 [Val]:   0%|          | 0/6234 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"001eb8bae261461289a2131f80b9ddf0"}},"metadata":{}},{"name":"stdout","text":"\nTRAIN METRICS:\nLoss: 0.1144 | Acc: 0.9080 | Prec: 0.9081 | Rec: 0.9080 | F1: 0.9080\n\nVALIDATION METRICS:\nLoss: 0.1725 | Acc: 0.8892 | Prec: 0.9445 | Rec: 0.8892 | F1: 0.9046\n\nConfusion Matrix:\n[[5078,   86]]  <- Real\n[[5437, 39264]]  <- Fake\nAverage Confidence: 0.4962\n🎉 NEW BEST MODEL SAVED! Val Acc: 0.8892\n\n==================================================\nEPOCH 2/5\n==================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2 [Train]:   0%|          | 0/3173 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fbec14247684c758350581e8505e427"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 2 [Val]:   0%|          | 0/6234 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b18fa0b1d2ea49d6a491b1fa30a12471"}},"metadata":{}},{"name":"stdout","text":"\nTRAIN METRICS:\nLoss: 0.0487 | Acc: 0.9651 | Prec: 0.9652 | Rec: 0.9651 | F1: 0.9651\n\nVALIDATION METRICS:\nLoss: 0.0427 | Acc: 0.9708 | Prec: 0.9758 | Rec: 0.9708 | F1: 0.9722\n\nConfusion Matrix:\n[[5041,  123]]  <- Real\n[[1332, 43369]]  <- Fake\nAverage Confidence: 0.5067\n🎉 NEW BEST MODEL SAVED! Val Acc: 0.9708\n\n==================================================\nEPOCH 3/5\n==================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3 [Train]:   0%|          | 0/3173 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6b908ed897842f1a2b4eb22c34068b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 3 [Val]:   0%|          | 0/6234 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c26016f80bcc4d6b812ff43f0606776e"}},"metadata":{}},{"name":"stdout","text":"\nTRAIN METRICS:\nLoss: 0.0304 | Acc: 0.9790 | Prec: 0.9790 | Rec: 0.9790 | F1: 0.9790\n\nVALIDATION METRICS:\nLoss: 0.0982 | Acc: 0.9375 | Prec: 0.9585 | Rec: 0.9375 | F1: 0.9432\n\nConfusion Matrix:\n[[5027,  137]]  <- Real\n[[2980, 41721]]  <- Fake\nAverage Confidence: 0.4990\n\n==================================================\nEPOCH 4/5\n==================================================\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4 [Train]:   0%|          | 0/3173 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fde1509e60064cc28921571be5488c16"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Epoch 4 [Val]:   0%|          | 0/6234 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e605f6bdf08246fb941e2e6094b139c4"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"import torch\n\n# Load the checkpoint\ncheckpoint = torch.load('best_improved_antispoofmodel.pth', map_location=device)\n\n# Load model state dict\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()  # Set to evaluation mode","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reinitialize the trainer (optional, if you want metrics/logging)\ntrainer = ImprovedTrainer(model=model, device=device)\n\n# Evaluate on the 30% subset\nval_loss, val_acc, val_prec, val_rec, val_f1, cm, confidences = trainer.validate_epoch(\n    val_loader=eval_loader_test, \n    epoch=0  # Epoch number doesn't matter for evaluation\n)\n\n# Print results\nprint(f\"\\nTEST RESULTS (30% EVAL SET):\")\nprint(f\"Loss: {val_loss:.4f} | Acc: {val_acc:.4f}\")\nprint(f\"Precision: {val_prec:.4f} | Recall: {val_rec:.4f} | F1: {val_f1:.4f}\")\nprint(\"\\nConfusion Matrix:\")\nprint(f\"[[{cm[0,0]:4d}, {cm[0,1]:4d}]]  <- Real (bonafide)\")\nprint(f\"[[{cm[1,0]:4d}, {cm[1,1]:4d}]]  <- Fake (spoof)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from prettytable import PrettyTable\n\ndef model_size_table(model):\n    table = PrettyTable([\"Layer\", \"Params\", \"Size (MB)\"])\n    total_params = 0\n    total_size = 0\n    \n    for name, param in model.named_parameters():\n        params = param.numel()\n        size = params * param.element_size() / (1024 ** 2)  # MB\n        table.add_row([name, f\"{params:,}\", f\"{size:.4f}\"])\n        total_params += params\n        total_size += size\n    \n    print(table)\n    print(f\"\\nTotal: {total_params:,} parameters, ~{total_size:.2f} MB\")\n\nmodel_size_table(model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), '/kaggle/working/antispoof_model.pth')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize a fresh model instance\nkmodel = AntiSpoofModel(\n     wav2vec_model_path=wav2vec_model_path + \"/wav2vec2-base\",\n    use_transformer=True,\n    num_classes=2\n).to('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Load the saved weights\nkmodel.load_state_dict(torch.load('/kaggle/working/antispoof_model.pth'))\nkmodel.eval()  # Set to evaluation mode\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess_audio(path, sample_rate=16000, max_len=48000):\n    waveform, sr = torchaudio.load(path)\n    if sr != sample_rate:\n        waveform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=sample_rate)(waveform)\n    waveform = waveform.mean(dim=0)  # Convert to mono\n    if waveform.shape[-1] < max_len:\n        pad = max_len - waveform.shape[-1]\n        waveform = F.pad(waveform, (0, pad))\n    else:\n        waveform = waveform[:max_len]\n    return waveform.unsqueeze(0)  # [1, T]\n\ndef predict_single(model, audio_path, device='cuda'):\n    model.eval()\n    waveform = preprocess_audio(audio_path).to(device)  # [1, T]\n    with torch.no_grad():\n        output = model(waveform)\n        probs = F.softmax(output['logits'], dim=1)\n        pred = torch.argmax(probs, dim=1).item()\n        confidence = output['confidence'].item()\n    \n    label = \"Real (Bonafide)\" if pred == 0 else \"Fake (Spoof)\"\n    return label, confidence\n\n\nfor path in test_dataset:\n    label, conf = predict_single(model, path)\n    print(f\"{path} => Prediction: {label}, Confidence: {conf:.2f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}